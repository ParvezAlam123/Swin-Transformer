{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128b498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7624e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right): # to handel the cyclic shift patches\n",
    "    mask = torch.zeros(window_size**2, window_size**2)\n",
    "    print(\"original mask: \", mask)\n",
    "    \n",
    "    if upper_lower:\n",
    "        mask[-displacement*window_size:, :-displacement * window_size] = float('-inf') # downleft section\n",
    "        mask[:-displacement * window_size, -displacement*window_size:] = float('-inf') # up right section \n",
    "        \n",
    "    if left_right:\n",
    "        mask = rearange(mask, '(h1 w1)(h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacment:] = float('inf')\n",
    "        mask = rearange(mask, 'h1, w1, h2, w2 -> (h1 w1)(h2 w2)')\n",
    "        \n",
    "    return mask \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        # dim=hidden_dim=(96,192,384,768)\n",
    "        # heads =num_heads=(3,6,12,24)\n",
    "        # head_dim=32\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads                    # (32 * 3=96, 32*6=192, 32*12=384, 32*24=768) = C\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim  * -0.5                   # scaling dot product inside the softmax \n",
    "        self.window_size = window_size                  # window_size=7\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted \n",
    "        \n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            \n",
    "            # (49, 49): masks are not learnable parameters: requires_grad=False\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False),\n",
    "                                                 requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True),\n",
    "                                                requires_grad=False)\n",
    "            \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        # dim= (96, 192, 384, 768) and (inner_dim = head_dim * heads) ; We can also use C*3 and gives us same thing\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))  # (49, 49)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        # inner_dim = head_dim * heads = C, dim = hidden_dim = (96, 192, 384, 768)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            #print('x.size: ', x.size())          # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "            x = self.cyclic_shift(x)\n",
    "            #print('x size', x.size())             # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "            \n",
    "        b, n_h, n_w, _, h = x.shape, self.heads           \n",
    "        #print('x shape: ', x.shape)               # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "        \n",
    "        #print('self.to_qkv(x): ', self.to_qkv(x).size())  # (1, (56, 28, 14, 7), (56, 28, 14, 7), (288, 576, 1152, 2304))  \n",
    "        \n",
    "        nw_h = n_h // self.window_size           \n",
    "        nw_w = n_w // seld.window_size \n",
    "        \n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                         h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "        \n",
    "        #print('q size: ', q.size())\n",
    "        #(b=1, h=(3,6,12,24), (nw_h*nw_w)=(64, 16,4,1),(w_h*w_w)=40, d=32) where d=head_dim, h=#heads\n",
    "        #print('k size: ', k.size())   # same sas the q\n",
    "        #print('v size: ', v.size())    # same as q \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                   nn.Linear(dim, hidden_dim),\n",
    "                   nn.GELU(),\n",
    "                   nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e57a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn \n",
    "        \n",
    "    def forward(seld, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63fe23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        # dim=hidden_dim=(96, 192, 384, 768)  # heads = num_heads=(3,6,12,24), mlp_dim=hidden_dim * 4\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                    heads=heads,\n",
    "                                                                    head_dim=head_dim,\n",
    "                                                                    shifted=shifted,\n",
    "                                                                    window_size=window_size,\n",
    "                                                relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)   # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "        x = self.mlp_block(x)         # (1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768))\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45dca974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_merge = nn.Conv(in_channels,\n",
    "                                  out_channels, \n",
    "                                  kernel_size=downscaling_factor,\n",
    "                                  stride=downscaling_factor, \n",
    "                                  padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('x.size: ', x.size())       # (1, (3, 96,192,384), (224, 56,28,14), (224, 56, 28,14))\n",
    "        #self.patch_merge(x)               # (1, (96, 192, 384, 768), (56, 28, 14, 7), (56, 28, 14, 7))\n",
    "        x = self.patch_merge(x).permute(0, 2, 3, 1) # (1, (56, 28, 14, 7), (56, 28,14,7), (96, 192, 384, 768))\n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44752718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim,\n",
    "                window_size, relative_pose_embedding):\n",
    "        super().__init__() \n",
    "        assert layers % 2 == 0 # stage layers need to  be divisible by 2 for regular and shifted block.\n",
    "        \n",
    "        self.patch_partition = PatchMerging_Conv(in_channels=in_channels, out_channels=hidden_dimension, \n",
    "                                                downscaling_factor = downscaling_factor)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimensin*4,\n",
    "                         shifted=False, window_size=window_size, relative_pose_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimensin, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                         shifted=True, window_size=window_size, retive_pos_embedding=relative_pos_embedding)\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #print('before patch merging: ', x.size())  # (1, (3,96,192,384), (224,56,28,14), (224,56,28,14))\n",
    "        x = self.patch_partition(x)\n",
    "        #print('after patch merging: ', x.size())  # (1, (56, 28, 14,7), (56,28,14,7),(96,192,384,768))\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)                    # (1, (56,28,14,7), (56,28,14,7), (96,192,384, 768))\n",
    "            x = shifted_block(x)                    # (1, (56,28,14,7), (56,28,14,7), (96,192,384,768))\n",
    "            \n",
    "        return x.permute(0,3,1,2)                   # (1, 768, 7, 7)\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32cc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                downscaling_factors=(4,2,2,2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0], \n",
    "                                  downscaling_factor=downscaling_factor[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pose_embedding=relative_pose_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dimension * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factor[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels = hidden_dim * 2, hidden_dimension = hidden_dim * 4,\n",
    "                                  layers = layers[2], downscaling_factor = downscaling_factors[2], \n",
    "                                  num_heads=heads[2], head_dim=head_dim, window_size=window_size,\n",
    "                                  relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                 downscaling_factor=downscaling_factors[3], num_heads=heads[3],\n",
    "                head_dim=head_dim, window_size=window_size, relative_pose_embedding=relative_pos_embedding)\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "                         nn.LayerNorm(hidden_dim * 8),\n",
    "                         nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        x = self.stage1(img)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = x.mean(dim=[2,3])\n",
    "        return self.mlp_head(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swin_t(hidden_dim=96, layers=(2,2,6,2), heads=(3,6,12,24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
